{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSxrWFpmLMHc"
      },
      "source": [
        "**Q1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df0xZSixZp-t",
        "outputId": "a7e840ff-c825-4402-f372-75a3c1416cc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigram Perplexity: 1065.285048562787\n",
            "Bigram (KN) Perplexity: 254.23441560040953\n",
            "Trigram (KN) Perplexity: 210.85815121363115\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# Data Loading and Preprocessing\n",
        "def read_file(path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        return [line.strip() for line in f if line.strip()]\n",
        "\n",
        "def tokenize_sentences(sentences):\n",
        "    return [['<s>'] + s.split() + ['</s>'] for s in sentences]\n",
        "\n",
        "def load_and_tokenize(path):\n",
        "    return tokenize_sentences(read_file(path))\n",
        "\n",
        "# Unigram Model with Dirichlet Smoothing \n",
        "class UnigramModel:\n",
        "    def __init__(self, alpha=0.1):\n",
        "        self.alpha = alpha\n",
        "        self.counts = Counter()\n",
        "        self.total = 0\n",
        "        self.vocab = set()\n",
        "\n",
        "    def train(self, corpus):\n",
        "        for sentence in corpus:\n",
        "            self.counts.update(sentence)\n",
        "        self.total = sum(self.counts.values())\n",
        "        self.vocab = set(self.counts.keys())\n",
        "\n",
        "    def prob(self, word):\n",
        "        return (self.counts[word] + self.alpha) / (self.total + self.alpha * len(self.vocab))\n",
        "\n",
        "    def perplexity(self, corpus):\n",
        "        log_prob = 0\n",
        "        word_count = 0\n",
        "        for sentence in corpus:\n",
        "            for word in sentence:\n",
        "                log_prob += math.log(self.prob(word))\n",
        "                word_count += 1\n",
        "        return math.exp(-log_prob / word_count)\n",
        "\n",
        "# N-Gram Model with Kneser-Ney Smoothing \n",
        "class NGramModelKN:\n",
        "    def __init__(self, n, discount=0.75):\n",
        "        self.n = n\n",
        "        self.discount = discount\n",
        "        self.ngram_counts = Counter()\n",
        "        self.prefix_counts = Counter()\n",
        "        self.continuation_counts = defaultdict(set)\n",
        "        self.unique_continuations = defaultdict(int)\n",
        "        self.lower_order_model = None\n",
        "        self.total_unique_ngrams = 0\n",
        "        self.total_vocab = set()\n",
        "\n",
        "    def train(self, corpus):\n",
        "        for sentence in corpus:\n",
        "            tokens = ['<s>'] * (self.n - 1) + sentence + ['</s>']\n",
        "            for i in range(len(tokens) - self.n + 1):\n",
        "                ngram = tuple(tokens[i:i + self.n])\n",
        "                prefix = ngram[:-1]\n",
        "                word = ngram[-1]\n",
        "                self.ngram_counts[ngram] += 1\n",
        "                self.prefix_counts[prefix] += 1\n",
        "                self.continuation_counts[word].add(prefix)\n",
        "                self.total_vocab.add(word)\n",
        "        self.total_unique_ngrams = len(self.ngram_counts)\n",
        "\n",
        "        #  Precompute how many unique words follow each prefix\n",
        "        for ngram in self.ngram_counts:\n",
        "            prefix = ngram[:-1]\n",
        "            self.unique_continuations[prefix] += 1\n",
        "\n",
        "        if self.n > 1:\n",
        "            self.lower_order_model = NGramModelKN(self.n - 1, self.discount)\n",
        "            self.lower_order_model.train(corpus)\n",
        "\n",
        "    def prob(self, ngram):\n",
        "        if self.n == 1:\n",
        "            word = ngram[0]\n",
        "            return len(self.continuation_counts[word]) / self.total_unique_ngrams if self.total_unique_ngrams > 0 else 1e-10\n",
        "\n",
        "        prefix, word = tuple(ngram[:-1]), ngram[-1]\n",
        "        count_ngram = self.ngram_counts[tuple(ngram)]\n",
        "        count_prefix = self.prefix_counts[prefix]\n",
        "\n",
        "        if count_prefix > 0:\n",
        "            lambda_factor = self.discount * self.unique_continuations[prefix] / count_prefix\n",
        "            lower_order_prob = self.lower_order_model.prob(ngram[1:])\n",
        "            return max(count_ngram - self.discount, 0) / count_prefix + lambda_factor * lower_order_prob\n",
        "        else:\n",
        "            return self.lower_order_model.prob(ngram[1:])\n",
        "\n",
        "    def perplexity(self, corpus):\n",
        "        log_prob = 0\n",
        "        word_count = 0\n",
        "        for sentence in corpus:\n",
        "            tokens = ['<s>'] * (self.n - 1) + sentence + ['</s>']\n",
        "            for i in range(self.n - 1, len(tokens)):\n",
        "                ngram = tokens[i - self.n + 1:i + 1]\n",
        "                p = self.prob(ngram)\n",
        "                log_prob += math.log(p if p > 0 else 1e-10)\n",
        "                word_count += 1\n",
        "        return math.exp(-log_prob / word_count)\n",
        "\n",
        "\n",
        "# Main Execution \n",
        "if __name__ == \"__main__\":\n",
        "    train = load_and_tokenize('/content/drive/MyDrive/Colab Notebooks/train.csv')\n",
        "    val = load_and_tokenize('/content/drive/MyDrive/Colab Notebooks/val.csv')\n",
        "    test = load_and_tokenize('/content/drive/MyDrive/Colab Notebooks/test.csv')\n",
        "\n",
        "    # Unigram model\n",
        "    unigram = UnigramModel(alpha=0.1)\n",
        "    unigram.train(train)\n",
        "    print(\"Unigram Perplexity:\", unigram.perplexity(test))\n",
        "\n",
        "    # Bigram model with Kneser-Ney\n",
        "    bigram = NGramModelKN(n=2)\n",
        "    bigram.train(train)\n",
        "    print(\"Bigram (KN) Perplexity:\", bigram.perplexity(test))\n",
        "\n",
        "    # Trigram model with Kneser-Ney\n",
        "    trigram = NGramModelKN(n=3)\n",
        "    trigram.train(train)\n",
        "    print(\"Trigram (KN) Perplexity:\", trigram.perplexity(test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG4z_j_IZp8X",
        "outputId": "cecd49d7-967d-4fb0-ec05-eb9c8972b554"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tuning Unigram Alpha:\n",
            "Alpha=0.0100 -> Val Perplexity=1150.65\n",
            "Alpha=0.0500 -> Val Perplexity=1089.19\n",
            "Alpha=0.1000 -> Val Perplexity=1064.47\n",
            "Alpha=0.2000 -> Val Perplexity=1041.47\n",
            "Alpha=0.5000 -> Val Perplexity=1016.14\n",
            "Alpha=1.0000 -> Val Perplexity=1005.28\n",
            "\n",
            "Best Alpha: 1.0000 with Val Perplexity: 1005.28\n",
            "\n",
            "Tuning Bigram Discount:\n",
            "Discount=0.10 (n=2) -> Val Perplexity=304.93\n",
            "Discount=0.20 (n=2) -> Val Perplexity=279.15\n",
            "Discount=0.35 (n=2) -> Val Perplexity=262.91\n",
            "Discount=0.45 (n=2) -> Val Perplexity=257.69\n",
            "Discount=0.50 (n=2) -> Val Perplexity=256.09\n",
            "Discount=0.60 (n=2) -> Val Perplexity=254.54\n",
            "Discount=0.75 (n=2) -> Val Perplexity=256.38\n",
            "Discount=0.90 (n=2) -> Val Perplexity=267.03\n",
            "Discount=1.00 (n=2) -> Val Perplexity=364.36\n",
            "\n",
            "Best Bigram Discount: 0.60 with Val Perplexity: 254.54\n",
            "\n",
            "Tuning Trigram Discount:\n",
            "Discount=0.10 (n=3) -> Val Perplexity=378.53\n",
            "Discount=0.20 (n=3) -> Val Perplexity=300.48\n",
            "Discount=0.35 (n=3) -> Val Perplexity=252.59\n",
            "Discount=0.45 (n=3) -> Val Perplexity=235.63\n",
            "Discount=0.50 (n=3) -> Val Perplexity=229.51\n",
            "Discount=0.60 (n=3) -> Val Perplexity=220.70\n",
            "Discount=0.75 (n=3) -> Val Perplexity=214.69\n",
            "Discount=0.90 (n=3) -> Val Perplexity=220.45\n",
            "Discount=1.00 (n=3) -> Val Perplexity=334.68\n",
            "\n",
            "Best Trigram Discount: 0.75 with Val Perplexity: 214.69\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameter Tuning \n",
        "\n",
        "def tune_unigram_alpha(train, val, alphas):\n",
        "    best_alpha = None\n",
        "    best_perplexity = float('inf')\n",
        "    for alpha in alphas:\n",
        "        model = UnigramModel(alpha=alpha)\n",
        "        model.train(train)\n",
        "        ppl = model.perplexity(val)\n",
        "        print(f\"Alpha={alpha:.4f} -> Val Perplexity={ppl:.2f}\")\n",
        "        if ppl < best_perplexity:\n",
        "            best_perplexity = ppl\n",
        "            best_alpha = alpha\n",
        "    return best_alpha, best_perplexity\n",
        "\n",
        "def tune_discount_ngram(train, val, n, discounts):\n",
        "    best_discount = None\n",
        "    best_perplexity = float('inf')\n",
        "    for d in discounts:\n",
        "        model = NGramModelKN(n=n, discount=d)\n",
        "        model.train(train)\n",
        "        ppl = model.perplexity(val)\n",
        "        print(f\"Discount={d:.2f} (n={n}) -> Val Perplexity={ppl:.2f}\")\n",
        "        if ppl < best_perplexity:\n",
        "            best_perplexity = ppl\n",
        "            best_discount = d\n",
        "    return best_discount, best_perplexity\n",
        "\n",
        "# Run Tuning \n",
        "print(\"\\nTuning Unigram Alpha:\")\n",
        "alpha_candidates = [0.01, 0.05, 0.1, 0.2, 0.5, 1.0]\n",
        "best_alpha, best_unigram_ppl = tune_unigram_alpha(train, val, alpha_candidates)\n",
        "print(f\"\\nBest Alpha: {best_alpha:.4f} with Val Perplexity: {best_unigram_ppl:.2f}\")\n",
        "\n",
        "print(\"\\nTuning Bigram Discount:\")\n",
        "discount_candidates = [0.1,0.2,0.35,0.45, 0.5,0.6, 0.75,0.9, 1.0]\n",
        "best_bigram_discount, best_bigram_ppl = tune_discount_ngram(train, val, 2, discount_candidates)\n",
        "print(f\"\\nBest Bigram Discount: {best_bigram_discount:.2f} with Val Perplexity: {best_bigram_ppl:.2f}\")\n",
        "\n",
        "print(\"\\nTuning Trigram Discount:\")\n",
        "best_trigram_discount, best_trigram_ppl = tune_discount_ngram(train, val, 3, discount_candidates)\n",
        "print(f\"\\nBest Trigram Discount: {best_trigram_discount:.2f} with Val Perplexity: {best_trigram_ppl:.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
